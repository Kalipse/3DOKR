# 3DOCKR

Conteneurisation d'une application de vote distribu√©e avec Docker

## Sommaire

1. [Introduction](#introduction)
2. [Installation](#installation)
3. [Conteneurisation des modules](#conteneurisation-des-modules)
4. [Cr√©ation du Docker Compose](#cr√©ation-du-docker-compose)
5. [D√©ploiement sur Docker Swarm](#d√©ploiement-sur-docker-swarm)
6. [Contributeurs](#contributeurs)

## Introduction

Ce mini-projet consiste √† moderniser le d√©ploiement d'une application distribu√©e de vote, en utilisant des conteneurs Docker. L'application permet √† un public de voter entre deux options et d'afficher les r√©sultats en temps r√©el. Actuellement, le projet est g√©r√© via des scripts bash, et l'objectif est de transformer ce processus en un environnement de conteneurs Docker pour une meilleure gestion, √©volutivit√© et facilit√© de d√©ploiement.

## Installation

L'installation du projet se fait √† partir de GitHub.

1. Vous devez commencer par cloner le projet

```bash
git clone https://github.com/Kalipse/3DOKR.git
```

2. Acc√©der au dossier du projet

```bash
cd /3DOKR
```

3. Et enfin ex√©cuter une commande Docker Compose.

```bash
docker compose up
```

## Conteneurisation des modules

Pour simplifier le d√©ploiement de l'application, nous avons d'abord converti les scripts bash initiaux associ√©s aux principaux modules (worker, result et vote) en Dockerfiles, en veillant √† respecter les meilleures pratiques. Cela nous a permis d'am√©liorer la gestion des conteneurs et d'assurer une plus grande coh√©rence.

En ce qui concerne les services Redis et PostgreSQL, nous avons choisi de les int√©grer directement dans notre fichier Docker Compose. Cette approche s'est av√©r√©e plus simple et plus efficace, car ces services n√©cessitaient peu de configurations sp√©cifiques et pouvaient √™tre rapidement mis en place dans l'environnement de conteneur.

Ce qui nous donne :

**Module vote (bash) :**

```bash
#!/usr/bin/env bash

# √âtape 2 - Vote
# Version de Python utilis√©e lors des d√©veloppements : 3.11

cd ../vote || exit

# Installation des d√©pendances
pip install -r requirements.txt

# Lancement du serveur
python app.py
```

üëá

**Module vote (Dockerfile) :**

```Dockerfile
# Dockerfile vote

# Utilisation de l'image Python version 3.11 comme base
FROM python:3.11-alpine

# D√©finition de la variable d'environnement PYTHON_ENV √† "production"
ENV PYTHON_ENV=production

# D√©finition du r√©pertoire de travail dans le conteneurv
WORKDIR /app

# Copie du fichier requirements.txt du r√©pertoire local dans le r√©pertoire de travail du conteneur
COPY requirements.txt /app/

RUN pip install -r requirements.txt ci --only=production

# Copie de tout le contenu du r√©pertoire local actuel dans le r√©pertoire de travail du conteneur
COPY --chown=user:user . .

# Exposition du port 8080 pour les connexions entrantes
EXPOSE 8080

# Commande √† ex√©cuter lorsque le conteneur est lanc√©
CMD ["python", "app.py"]
```

<br><br><br>

**Module worker (bash) :**

```bash
#!/usr/bin/env bash

# √âtape 4 - Worker
# Version de .NET Core utilis√©e lors des d√©veloppements : 7.0

cd ../worker || exit

# Installation des d√©pendances
dotnet restore

# Production d'un ex√©cutable
dotnet publish -c release --self-contained false --no-restore

# Lancement du serveur
dotnet bin/release/net7.0/Worker.dll
```

üëá

**Module worker (Dockerfile) :**

```Dockerfile
# Dockerfile worker

# Utilisation de l'image .NET SDK version 7.0 comme base pour la construction
FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build

# D√©finition de la variable d'environnement DOTNET_ENV √† "production"
ENV DOTNET_ENV=production

# D√©finition du r√©pertoire de travail dans le conteneur
WORKDIR /app

# Copie de tout le contenu du r√©pertoire local actuel dans le r√©pertoire de travail du conteneur
COPY --chown=user:user . ./

# Restauration des d√©pendances du projet
RUN dotnet restore

# Publication du projet en mode "release" sans restauration
RUN dotnet publish -c release --self-contained false --no-restore

# Utilisation de l'image .NET Runtime version 7.0 comme base pour l'ex√©cution
FROM mcr.microsoft.com/dotnet/runtime:7.0 AS build2

# D√©finition du r√©pertoire de travail dans le conteneur
WORKDIR /app

# Copie des fichiers publi√©s √† partir de la premi√®re √©tape dans le r√©pertoire de travail de cette √©tape
COPY --from=build --chown=user:user /app/bin/release/net7.0 ./

# Commande √† ex√©cuter lorsque le conteneur est lanc√©
CMD ["dotnet", "Worker.dll"]
```

<br><br><br>

**Module result (bash) :**

```bash
#!/usr/bin/env bash

# √âtape 5 - Result
# Version de Node.js utilis√©e lors des d√©veloppements : 18

cd ../result || exit

# Installation des d√©pendances
npm install

# Lancement du serveur
npm start
```

üëá

**Module result (Dockerfile) :**

```Dockerfile
# Dockerfile result

# Utilisation de l'image Node.js version 18 comme base
FROM node:18-alpine

# D√©finition de la variable d'environnement NODE_ENV √† "production"
ENV NODE_ENV production

# D√©finition du r√©pertoire de travail dans le conteneur
WORKDIR /app

# Copie de tout le contenu du r√©pertoire local actuel dans le r√©pertoire de travail du conteneur
COPY --chown=user:user . /app

# Copie du r√©pertoire local "views" dans le r√©pertoire de travail du conteneur
COPY --chown=user:user ./views /app

# Configuration du cache npm en utilisant un point de montage
# pour am√©liorer la vitesse des installations ult√©rieures
RUN --mount=type=cache,target=/usr/src/app/.npm \
  npm set cache /usr/src/app/.npm && \
  npm install ci --only=production

# Exposition du port 8888 pour les connexions entrantes
EXPOSE 8888
# Commande √† ex√©cuter lorsque le conteneur est lanc√©
CMD ["node", "server.js"]
```

<br><br><br>

Nous avons √©galement introduit un Dockerfile 'User',qui nous permet de ne pas ex√©cuter les conteneurs en tant qu'utilisateur 'root'. Cette approche renforce la s√©curit√© de notre application en √©vitant l'ex√©cution de processus sous un privil√®ge √©lev√©

```Dockerfile
# Dockerfile user

# Utilisation de l'image Debian Bullseye Slim comme base
FROM debian:bullseye-slim

# Cr√©ation d'un utilisateur "user" avec un shell par d√©faut (/bin/bash)
RUN useradd -ms /bin/bash user

# D√©finition de l'utilisateur "user" comme utilisateur par d√©faut pour les instructions suivantes
USER user

# D√©finition du r√©pertoire de travail dans le conteneur pour l'utilisateur "user"
WORKDIR /home/user

# Commande √† ex√©cuter lorsque le conteneur est lanc√© (dans ce cas, "tail -f /dev/null" pour maintenir le conteneur actif)
CMD ["tail", "-f", "/dev/null"]
```

Dans nos fichiers Dockerfile, nous pouvons observer l'application de plusieurs pratiques recommand√©es, notamment :

- L'utilisation d'une image de base l√©g√®re

```Dockerfile
FROM node:18-alpine
FROM debian:bullseye-slim
FROM python:3.11-alpine
```

- La d√©finition de l'environnement de production

```Dockerfile
ENV NODE_ENV production
ENV DOTNET_ENV=production
ENV PYTHON_ENV=production
```

- La d√©finition du r√©pertoire de travail

```Dockerfile
WORKDIR /home/user
WORKDIR /app
```

- La copie du contenu local

```Dockerfile
COPY --chown=user:user ./views /app
COPY --from=build --chown=user:user /app/bin/release/net7.0 ./
COPY requirements.txt /app/
```

- La configuration du cache npm

```Dockerfile
RUN --mount=type=cache,target=/usr/src/app/.npm \
npm set cache /usr/src/app/.npm && \
npm install ci --only=production
```

- L'exposition du port

```Dockerfile
EXPOSE 8080
EXPOSE 8888
```

- La gestion des droits d'utilisateur avec `--chown=user:user`

## Cr√©ation du Docker Compose

Nous avons √©labor√© un fichier Docker Compose regroupant les trois modules principaux (result, vote, worker), ainsi que les services Redis et PostgreSQL. Nous avons √©galement inclus notre module additionnel, 'user'.

```YAML
# Docker compose

services:
  # Service Redis utilis√© pour le stockage en cache
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-volume:/data
    healthcheck:
      test: ["CMD", "sh", "-c", "redis-cli ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Service PostgreSQL utilis√© pour la base de donn√©es
  postgres:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

  # Service utilisateur personnalis√©
  user:
    build:
      context: .
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-user

  # Service de vote
  vote:
    build:
      context: ./vote
    ports:
      - "8080:8080"
    depends_on:
      - redis
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Worker service qui d√©pend de Redis et PostgreSQL
  worker:
    build:
      context: ./worker
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis
      - network-postgres

  # Service de r√©sultat qui d√©pend du worker
  result:
    build:
      context: ./result
    ports:
      - "8081:8888"
    depends_on:
      - worker
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8888 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

volumes:
  redis-volume:
    # Volume pour stocker les donn√©es Redis
  postgres-volume:
    # Volume pour stocker les donn√©es PostgreSQL

networks:
  network-user:
    # R√©seau personnalis√© pour le service utilisateur
  network-redis:
    # R√©seau personnalis√© pour les services Redis et Vote
  network-postgres:
    # R√©seau personnalis√© pour les services PostgreSQL, Worker, et Result
```

En plus d'int√©grer nos services, nous avons apport√© quatre am√©liorations pour optimiser davantage notre composition.

1. R√©seaux :

- Les r√©seaux personnalis√©s nous permettront d'organiser nos conteneurs Docker, permettant ainsi aux conteneurs de communiquer entre eux tout en les isolant les uns des autres.

2. Healthcheck :

- Les tests de sant√© nous permettront de surveiller l'√©tat des conteneurs et de les relancer si n√©cessaire. Ces tests seront r√©alis√©s √† intervalles r√©guliers et avec un nombre d'essais d√©fini.

3. Volumes :

- Les volumes nous permettront de conserver nos donn√©es en m√©moire m√™me si le conteneur est supprim√©.

4. Depends_on :

- L'attribut depends_on indique √† Docker quels conteneurs doivent √™tre op√©rationnels avant que d'autres puissent d√©marrer. Dans le code source initial, les scripts Bash devaient √™tre ex√©cut√©s dans un ordre sp√©cifique pour garantir le fonctionnement de l'application. Ici, nous d√©finissons cet ordre en utilisant cet attribut.

## D√©ploiement sur Docker Swarm

Pour d√©ployer l'application sur Docker Swarm, nous avons utilis√© Vagrant pour g√©rer efficacement et rapidement les machines virtuelles.

Dans un premier temps, nous avons cr√©√© un fichier Vagrant qui a permis de :

- Configurer notre manager et nos deux workers
- Initialiser notre cluster Docker Swarm
- R√©cup√©rer le dossier du projet depuis GitHub
- Construire nos diff√©rentes images

```Ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

NODES = {
 "manager1" => "192.168.99.100",
 "worker1" => "192.168.99.101",
 "worker2" => "192.168.99.102",
}

Vagrant.configure("2") do |config|
 NODES.each do |(node_name, ip_address)|
   config.vm.define node_name do |node|
     node.vm.box = "bento/ubuntu-20.04"
     node.vm.hostname = node_name
     node.vm.network "private_network", ip: ip_address

     node.vm.provider "virtualbox" do |v|
       v.name = node_name
       v.memory = "1024"
       v.cpus = "1"
     end

     node.vm.provision "shell", inline: <<-SHELL
       # Faire en sorte que les machines puissent communiquer entre elles via leur hostnames (exemple: ping worker1 depuis manager1)
       #{NODES.map{ |n_name, ip| "echo '#{ip} #{n_name}' | sudo tee -a /etc/hosts\n"}.join}

       # Installer Docker
       curl -fsSL get.docker.com -o get-docker.sh
       CHANNEL=stable sh get-docker.sh
       rm get-docker.sh

       # Faire en sorte que le daemon Docker soit accessible depuis l'h√¥te
       sudo mkdir -p /etc/systemd/system/docker.service.d
       sudo bash -c 'echo -e "[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375" > /etc/systemd/system/docker.service.d/options.conf'
       sudo systemctl daemon-reload
       sudo systemctl restart docker.service

     SHELL
     if node_name == "manager1"
       node.vm.provision "shell", inline: <<-SHELL
         # Cloner le d√©p√¥t Git
         git clone
         git clone https://github.com/Kalipse/3DOKR.git /3DOKR
         cd /3DOKR

         #build docker image
         docker build -t vote-service ./vote
         docker build -t worker-service ./worker
         docker build -t result-service ./result

         #initialiser le swarm
         docker swarm init --advertise-addr 192.168.99.100

         #il faut ensuite que les workers rejoignent le swarm

         #on peut ensuite deployer les services
         #docker stack deploy --compose-file docker-compose.yml 3DOKR


       SHELL
     end



   end
 end
end
```

Ensuite, nous acc√©dons √† chaque worker pour les rejoindre au cluster Docker Swarm.

```bash
vagrant ssh worker1
```

```bash
sudo docker swarm join --token INSERER LE TOKEN + ADDRESSE IP
```

```bash
vagrant ssh worker2
```

```bash
sudo docker swarm join --token INSERER LE TOKEN + ADDRESSE IP
```

Nous modifions ensuite le Docker Compose pr√©sent dans le manager pour y inclure l'attribut "deploy" qui contient :

Le nombre de r√©plicas souhait√©, permettant ainsi de b√©n√©ficier d'une haute disponibilit√© et d'une tol√©rance aux pannes.
La politique de red√©marrage, permettant de d√©finir une condition de d√©marrage pour nos diff√©rents services.

```bash
vagrant ssh manager1
```

```YAML
#Docker compose Swarm

version: "3.7"

services:
  # Service Redis utilis√© pour le stockage en cache
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-volume:/data
    healthcheck:
      test: ["CMD", "sh", "-c", "redis-cli ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Service PostgreSQL utilis√© pour la base de donn√©es
  postgres:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

  # Service utilisateur personnalis√©
  user:
    image: user-service:latest
    ports:
      - "8888:8888"
    depends_on:
      - redis
      - postgres
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-user

  # Service de vote
  vote:
    image: vote-service:latest
    ports:
      - "8080:8080"
    depends_on:
      - redis
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Worker service qui d√©pend de Redis et PostgreSQL
  worker:
    image: worker-service:latest
    depends_on:
      - redis
      - postgres
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis
      - network-postgres

  # Service de r√©sultat qui d√©pend du worker
  result:
    image: result-service:latest
    ports:
      - "8081:8888"
    depends_on:
      - worker
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8888 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

volumes:
  redis-volume:
    # Volume pour stocker les donn√©es Redis
  postgres-volume:
    # Volume pour stocker les donn√©es PostgreSQL

networks:
  network-user:
    # R√©seau personnalis√© pour le service utilisateur
  network-redis:
    # R√©seau personnalis√© pour les services Redis et Vote
  network-postgres:
    # R√©seau personnalis√© pour les services PostgreSQL, Worker et Result
```

Il ne nous reste plus qu'√† acc√©der au manager et d√©ployer notre application sur le Docker Swarm.

```bash
docker stack deploy -c SwarmCompose.yaml 3DOKR
```

**Remarque :**

Nous avons aussi modifier les differents noms de services pour y permettre la connexion.

comme par exemple :

```javascript
const pool = new pg.Pool({
  connectionString: "postgres://postgres:postgres@3DOKR_postgres/postgres",
});
```

ou

```cs
var redisConn = OpenRedisConnection("3DOKR_redis");
```

Notre d√©ploiement de Swarm est d√©sormais op√©rationnel, ce qui renforce consid√©rablement l'infrastructure de notre application. Cette mise en place nous conf√®re une robustesse am√©lior√©e. Elle nous permet √©galement d'assurer une gestion plus efficace de notre application, offrant une haute disponibilit√© et une tol√©rance aux pannes.

## Contributeurs

Ce projet a √©t√© realis√© par :

[Noah](https://github.com/Kaalipse)
et
[Francois Xavier](https://github.com/francoisxavierdesaintjean)
