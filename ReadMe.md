# 3DOCKR

Conteneurisation d'une application de vote distribuÃ©e avec Docker

## Sommaire

1. [Introduction](#introduction)
2. [Installtion](#installation)
3. [Conteneurisation des modules](#conteneurisation-des-modules)
4. [CrÃ©ation du Docker Compose](#crÃ©ation-du-docker-compose)
5. [DÃ©ploiement sur Docker Swarm](#dÃ©ploiement-sur-docker-swarm)
6. [Contributeurs](#contributeur)

## Introduction

Ce mini-projet consiste Ã  moderniser le dÃ©ploiement d'une application distribuÃ©e de vote, en utilisant des conteneurs Docker. L'application permet Ã  un public de voter entre deux options et d'afficher les rÃ©sultats en temps rÃ©el. Actuellement, le projet est gÃ©rÃ© via des scripts bash, et l'objectif est de transformer ce processus en un environnement de conteneurs Docker pour une meilleure gestion, Ã©volutivitÃ© et facilitÃ© de dÃ©ploiement.

## Installation

L'installation du projet se fait Ã  partir de GitHub.

1. Vous devez commencer par cloner le projet

```bash
git clone https://github.com/Kalipse/3DOKR.git /3DOKR
```

2. AccÃ©der au dossier du projet

```bash
cd /3DOKR
```

3. Et enfin exÃ©cuter une commande Docker Compose.

```bash
docker compose up
```

## Conteneurisation des modules

Pour simplifier le dÃ©ploiement de l'application, nous avons d'abord converti les scripts bash initiaux associÃ©s aux principaux modules (worker, result et vote) en Dockerfiles, en veillant Ã  respecter les meilleures pratiques. Cela nous a permis d'amÃ©liorer la gestion des conteneurs et d'assurer une plus grande cohÃ©rence.

En ce qui concerne les services Redis et PostgreSQL, nous avons choisi de les intÃ©grer directement dans notre fichier Docker Compose. Cette approche s'est avÃ©rÃ©e plus simple et plus efficace, car ces services nÃ©cessitaient peu de configurations spÃ©cifiques et pouvaient Ãªtre rapidement mis en place dans l'environnement de conteneur.

Ce qui nous donne :

**Module vote (bash) :**

```bash
#!/usr/bin/env bash

# Ã‰tape 2 - Vote
# Version de Python utilisÃ©e lors des dÃ©veloppements : 3.11

cd ../vote || exit

# Installation des dÃ©pendances
pip install -r requirements.txt

# Lancement du serveur
python app.py
```

ðŸ‘‡

**Module vote (Dockerfile) :**

```Dockerfile
# Dockerfile vote

# Utilisation de l'image Python version 3.11 comme base
FROM python:3.11-alpine

# DÃ©finition de la variable d'environnement PYTHON_ENV Ã  "production"
ENV PYTHON_ENV=production

# DÃ©finition du rÃ©pertoire de travail dans le conteneurv
WORKDIR /app

# Copie du fichier requirements.txt du rÃ©pertoire local dans le rÃ©pertoire de travail du conteneur
COPY requirements.txt /app/

RUN pip install -r requirements.txt ci --only=production

# Copie de tout le contenu du rÃ©pertoire local actuel dans le rÃ©pertoire de travail du conteneur
COPY --chown=user:user . .

# Exposition du port 8080 pour les connexions entrantes
EXPOSE 8080

# Commande Ã  exÃ©cuter lorsque le conteneur est lancÃ©
CMD ["python", "app.py"]
```

**Module worker (bash) :**

```bash
#!/usr/bin/env bash

# Ã‰tape 4 - Worker
# Version de .NET Core utilisÃ©e lors des dÃ©veloppements : 7.0

cd ../worker || exit

# Installation des dÃ©pendances
dotnet restore

# Production d'un exÃ©cutable
dotnet publish -c release --self-contained false --no-restore

# Lancement du serveur
dotnet bin/release/net7.0/Worker.dll
```

ðŸ‘‡

**Module worker (Dockerfile) :**

```Dockerfile
# Dockerfile worker

# Utilisation de l'image .NET SDK version 7.0 comme base pour la construction
FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build

# DÃ©finition de la variable d'environnement DOTNET_ENV Ã  "production"
ENV DOTNET_ENV=production

# DÃ©finition du rÃ©pertoire de travail dans le conteneur
WORKDIR /app

# Copie de tout le contenu du rÃ©pertoire local actuel dans le rÃ©pertoire de travail du conteneur
COPY --chown=user:user . ./

# Restauration des dÃ©pendances du projet
RUN dotnet restore

# Publication du projet en mode "release" sans restauration
RUN dotnet publish -c release --self-contained false --no-restore

# Utilisation de l'image .NET Runtime version 7.0 comme base pour l'exÃ©cution
FROM mcr.microsoft.com/dotnet/runtime:7.0 AS build2

# DÃ©finition du rÃ©pertoire de travail dans le conteneur
WORKDIR /app

# Copie des fichiers publiÃ©s Ã  partir de la premiÃ¨re Ã©tape dans le rÃ©pertoire de travail de cette Ã©tape
COPY --from=build --chown=user:user /app/bin/release/net7.0 ./

# Commande Ã  exÃ©cuter lorsque le conteneur est lancÃ©
CMD ["dotnet", "Worker.dll"]
```

**Module result (bash) :**

```bash
#!/usr/bin/env bash

# Ã‰tape 5 - Result
# Version de Node.js utilisÃ©e lors des dÃ©veloppements : 18

cd ../result || exit

# Installation des dÃ©pendances
npm install

# Lancement du serveur
npm start
```

ðŸ‘‡

**Module result (Dockerfile) :**

```Dockerfile
# Dockerfile result

# Utilisation de l'image Node.js version 18 comme base
FROM node:18-alpine

# DÃ©finition de la variable d'environnement NODE_ENV Ã  "production"
ENV NODE_ENV production

# DÃ©finition du rÃ©pertoire de travail dans le conteneur
WORKDIR /app

# Copie de tout le contenu du rÃ©pertoire local actuel dans le rÃ©pertoire de travail du conteneur
COPY --chown=user:user . /app

# Copie du rÃ©pertoire local "views" dans le rÃ©pertoire de travail du conteneur
COPY --chown=user:user ./views /app

# Configuration du cache npm en utilisant un point de montage
# pour amÃ©liorer la vitesse des installations ultÃ©rieures
RUN --mount=type=cache,target=/usr/src/app/.npm \
  npm set cache /usr/src/app/.npm && \
  npm install ci --only=production

# Exposition du port 8888 pour les connexions entrantes
EXPOSE 8888
# Commande Ã  exÃ©cuter lorsque le conteneur est lancÃ©
CMD ["node", "server.js"]
```

Nous avons Ã©galement introduit un Dockerfile 'User',qui nous permet de ne pas exÃ©cuter les conteneurs en tant qu'utilisateur 'root'. Cette approche renforce la sÃ©curitÃ© de notre application en Ã©vitant l'exÃ©cution de processus sous un privilÃ¨ge Ã©levÃ©

```Dockerfile
# Dockerfile user

# Utilisation de l'image Debian Bullseye Slim comme base
FROM debian:bullseye-slim

# CrÃ©ation d'un utilisateur "user" avec un shell par dÃ©faut (/bin/bash)
RUN useradd -ms /bin/bash user

# DÃ©finition de l'utilisateur "user" comme utilisateur par dÃ©faut pour les instructions suivantes
USER user

# DÃ©finition du rÃ©pertoire de travail dans le conteneur pour l'utilisateur "user"
WORKDIR /home/user

# Commande Ã  exÃ©cuter lorsque le conteneur est lancÃ© (dans ce cas, "tail -f /dev/null" pour maintenir le conteneur actif)
CMD ["tail", "-f", "/dev/null"]
```

Dans nos fichiers Dockerfile, nous pouvons observer l'application de plusieurs pratiques recommandÃ©es, notamment :

- L'utilisation d'une image de base lÃ©gÃ¨re

```Dockerfile
FROM node:18-alpine
FROM debian:bullseye-slim
FROM python:3.11-alpine
```

- La dÃ©finition de l'environnement de production

```Dockerfile
ENV NODE_ENV production
ENV DOTNET_ENV=production
ENV PYTHON_ENV=production
```

- La dÃ©finition du rÃ©pertoire de travail

```Dockerfile
WORKDIR /home/user
WORKDIR /app
```

- La copie du contenu local

```Dockerfile
COPY --chown=user:user ./views /app
COPY --from=build --chown=user:user /app/bin/release/net7.0 ./
COPY requirements.txt /app/
```

- La configuration du cache npm

```Dockerfile
RUN --mount=type=cache,target=/usr/src/app/.npm \
npm set cache /usr/src/app/.npm && \
npm install ci --only=production
```

- L'exposition du port

```Dockerfile
EXPOSE 8080
EXPOSE 8888
```

- La gestion des droits d'utilisateur avec `--chown=user:user`

## CrÃ©ation du Docker Compose

Nous avons Ã©laborÃ© un fichier Docker Compose regroupant les trois modules principaux (result, vote, worker), ainsi que les services Redis et PostgreSQL. Nous avons Ã©galement inclus notre module additionnel, 'user'.

```YAML
# Docker compose

services:
  # Service Redis utilisÃ© pour le stockage en cache
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-volume:/data
    healthcheck:
      test: ["CMD", "sh", "-c", "redis-cli ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Service PostgreSQL utilisÃ© pour la base de donnÃ©es
  postgres:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

  # Service utilisateur personnalisÃ©
  user:
    build:
      context: .
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-user

  # Service de vote
  vote:
    build:
      context: ./vote
    ports:
      - "8080:8080"
    depends_on:
      - redis
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Worker service qui dÃ©pend de Redis et PostgreSQL
  worker:
    build:
      context: ./worker
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis
      - network-postgres

  # Service de rÃ©sultat qui dÃ©pend du worker
  result:
    build:
      context: ./result
    ports:
      - "8081:8888"
    depends_on:
      - worker
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8888 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

volumes:
  redis-volume:
    # Volume pour stocker les donnÃ©es Redis
  postgres-volume:
    # Volume pour stocker les donnÃ©es PostgreSQL

networks:
  network-user:
    # RÃ©seau personnalisÃ© pour le service utilisateur
  network-redis:
    # RÃ©seau personnalisÃ© pour les services Redis et Vote
  network-postgres:
    # RÃ©seau personnalisÃ© pour les services PostgreSQL, Worker, et Result
```

En plus d'intÃ©grer nos services, nous avons apportÃ© quatre amÃ©liorations pour optimiser davantage notre composition.

1. RÃ©seaux :

- Les rÃ©seaux personnalisÃ©s nous permettront d'organiser nos conteneurs Docker, permettant ainsi aux conteneurs de communiquer entre eux tout en les isolant les uns des autres.

2. Healthcheck :

- Les tests de santÃ© nous permettront de surveiller l'Ã©tat des conteneurs et de les relancer si nÃ©cessaire. Ces tests seront rÃ©alisÃ©s Ã  intervalles rÃ©guliers et avec un nombre d'essais dÃ©fini.

3. Volumes :

- Les volumes nous permettront de conserver nos donnÃ©es en mÃ©moire mÃªme si le conteneur est supprimÃ©.

4. Depends_on :

- L'attribut depends_on indique Ã  Docker quels conteneurs doivent Ãªtre opÃ©rationnels avant que d'autres puissent dÃ©marrer. Dans le code source initial, les scripts Bash devaient Ãªtre exÃ©cutÃ©s dans un ordre spÃ©cifique pour garantir le fonctionnement de l'application. Ici, nous dÃ©finissons cet ordre en utilisant cet attribut.

## DÃ©ploiement sur Docker Swarm

Pour dÃ©ployer l'application sur Docker Swarm, nous avons utilisÃ© Vagrant pour gÃ©rer efficacement et rapidement les machines virtuelles.

Dans un premier temps, nous avons crÃ©Ã© un fichier Vagrant qui a permis de :

- Configurer notre manager et nos deux workers
- Initialiser notre cluster Docker Swarm
- RÃ©cupÃ©rer le dossier du projet depuis GitHub
- Construire nos diffÃ©rentes images

```Ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

NODES = {
 "manager1" => "192.168.99.100",
 "worker1" => "192.168.99.101",
 "worker2" => "192.168.99.102",
}

Vagrant.configure("2") do |config|
 NODES.each do |(node_name, ip_address)|
   config.vm.define node_name do |node|
     node.vm.box = "bento/ubuntu-20.04"
     node.vm.hostname = node_name
     node.vm.network "private_network", ip: ip_address

     node.vm.provider "virtualbox" do |v|
       v.name = node_name
       v.memory = "1024"
       v.cpus = "1"
     end

     node.vm.provision "shell", inline: <<-SHELL
       # Faire en sorte que les machines puissent communiquer entre elles via leur hostnames (exemple: ping worker1 depuis manager1)
       #{NODES.map{ |n_name, ip| "echo '#{ip} #{n_name}' | sudo tee -a /etc/hosts\n"}.join}

       # Installer Docker
       curl -fsSL get.docker.com -o get-docker.sh
       CHANNEL=stable sh get-docker.sh
       rm get-docker.sh

       # Faire en sorte que le daemon Docker soit accessible depuis l'hÃ´te
       sudo mkdir -p /etc/systemd/system/docker.service.d
       sudo bash -c 'echo -e "[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375" > /etc/systemd/system/docker.service.d/options.conf'
       sudo systemctl daemon-reload
       sudo systemctl restart docker.service

     SHELL
     if node_name == "manager1"
       node.vm.provision "shell", inline: <<-SHELL
         # Cloner le dÃ©pÃ´t Git
         git clone
         git clone https://github.com/Kalipse/3DOKR.git /3DOKR
         cd /3DOKR

         #build docker image
         docker build -t vote-service ./vote
         docker build -t worker-service ./worker
         docker build -t result-service ./result

         #initialiser le swarm
         docker swarm init --advertise-addr 192.168.99.100

         #il faut ensuite que les workers rejoignent le swarm

         #on peut ensuite deployer les services
         #docker stack deploy --compose-file docker-compose.yml 3DOKR


       SHELL
     end



   end
 end
end
```

Ensuite, nous accÃ©dons Ã  chaque worker pour les rejoindre au cluster Docker Swarm.

```bash
vagrant ssh worker1
```

```bash
sudo docker swarm join --token INSERER LE TOKEN + ADDRESSE IP
```

```bash
vagrant ssh worker2
```

```bash
sudo docker swarm join --token INSERER LE TOKEN + ADDRESSE IP
```

Nous modifions ensuite le compose present dans le manager pour inclure l'attribut deploy

```bash
vagrant ssh manager1
```

```YAML
#Docker compose Swarm

version: "3.7"

services:
  # Service Redis utilisÃ© pour le stockage en cache
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-volume:/data
    healthcheck:
      test: ["CMD", "sh", "-c", "redis-cli ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Service PostgreSQL utilisÃ© pour la base de donnÃ©es
  postgres:
    image: postgres:alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

  # Service utilisateur personnalisÃ©
  user:
    image: user-service:latest
    ports:
      - "8888:8888"
    depends_on:
      - redis
      - postgres
    deploy:
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-user

  # Service de vote
  vote:
    image: vote-service:latest
    ports:
      - "8080:8080"
    depends_on:
      - redis
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis

  # Worker service qui dÃ©pend de Redis et PostgreSQL
  worker:
    image: worker-service:latest
    depends_on:
      - redis
      - postgres
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-redis
      - network-postgres

  # Service de rÃ©sultat qui dÃ©pend du worker
  result:
    image: result-service:latest
    ports:
      - "8081:8888"
    depends_on:
      - worker
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:8888 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - network-postgres

volumes:
  redis-volume:
    # Volume pour stocker les donnÃ©es Redis
  postgres-volume:
    # Volume pour stocker les donnÃ©es PostgreSQL

networks:
  network-user:
    # RÃ©seau personnalisÃ© pour le service utilisateur
  network-redis:
    # RÃ©seau personnalisÃ© pour les services Redis et Vote
  network-postgres:
    # RÃ©seau personnalisÃ© pour les services PostgreSQL, Worker et Result
```

Il ne nous reste plus qu'Ã  accÃ©der au manager et dÃ©ployer notre application sur le Docker Swarm.

```bash
docker stack deploy -c SwarmCompose.yaml 3DOKR
```

**Remarque :**

Nous avons aussi modifier les differents noms de services pour y permettre la connexion.

comme par exemple :

```javascript
const pool = new pg.Pool({
  connectionString: "postgres://postgres:postgres@3DOKR_postgres/postgres",
});
```

ou

```cs
var redisConn = OpenRedisConnection("3DOKR_redis");
```

## Contributeurs

Ce projet a Ã©tÃ© realisÃ© par :

[Noah](https://github.com/Kaalipse)
Et
[Francois Xavier](https://github.com/francoisxavierdesaintjean)
